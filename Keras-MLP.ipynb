{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "*Introduction*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Keras is a Python library for quickly building neural networks with either TensorFlow or Theano. In its developers' own words, it was created \"with a focus on enabling fast experimentation\".\n\nTensorFlow and Theano offer low-level control over a network's implementation, but (at the moment) many ML problems can be solved with a 'standard' architecture such as a CNN or an LSTM. Keras provides a means of rapidly describing a network, allowing its users to focus on tuning rather than reinventing the wheel.\n\nIn this notebook, I'll outline how to build a 3-layer multilayer perceptron using Keras. If you're unsure what a multilayer perceptron is, I'd recommend Michael Nielsen's book as an introduction (http://neuralnetworksanddeeplearning.com/).\n\nI'll be using Theano as a backend, but you could equally well as use TensorFlow - as far as I'm aware, the Keras code written can be the same!",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Load libraries*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport theano\n%env KERAS_BACKEND=theano\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Import the data*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mnist = pd.read_csv('../input/train.csv')\nX = mnist.drop('label', axis=1).as_matrix() # 784 features per sample (28x28 grid of pixels)\nX = (X - X.mean(axis=0)) # Standardize input features (http://www.faqs.org/faqs/ai-faq/neural-nets/part2/)\ny = mnist.ix[:, 'label'] # 10 response classes (digits 0-9)\ny = pd.get_dummies(y).as_matrix() # One-hot encode response\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Graph the network*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Dense(30, input_shape=(784,),\n                init='normal')) # 50 neurons in 1st layer, each receiving all 784 inputs\nmodel.add(Activation('relu'))\nmodel.add(Dense(30, init='normal')) # 50 neurons in 2nd layer, each receiving all 50 outputs from the 1st layer\nmodel.add(Activation('relu'))\nmodel.add(Dense(10, init='normal'))\nmodel.add(Activation('softmax'))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In the step above we build a graph of the neural network. If you've seen any raw TensorFlow models before, I'm sure you'll appreciate how succinct Keras is. A few pointers:\n\n - A *Sequential* model is a run-of-the-mill feedforward neural network. Neurons in a layer are only connected to neurons in the layers directly ahead of or behind them.\n - It's not necessary to specify the batch size (i.e. the number of input samples). *`input_shape=(784,)`*  is equivalent to *`batch_input_shape=(None, 784)`*. *\"None\"* indicates that the network can handle the number of input samples flexibly.\n - A *Dense* layer is fully connected to the previous layer - each of its neurons receive all of the previous layer's outputs.\n - The activation functions can also be specified when creating a layer. For example, it's possible to write lines 2 and 3 in the above as *model.add(Dense(50, input_shape=(784,), init='normal', activation='relu'))*.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<tiny> Short aside: This network isn't *strictly* an MLP - the activation function of a perceptron is defined as the step function (here we use ReLU) </tiny>",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Assert how the model learns*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "When happy with the model's structure, call *model.compile()* with the loss function, optimization method, and metrics you would like to see while the model is being fit. In so doing, you define how the model will learn. Below I also create a stochastic gradient descent optimizer object to encourage the model to fit more quickly.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Fit the model!*",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model.fit(X.as_matrix(), y.as_matrix(),\n          nb_epoch=10, batch_size=25, verbose=2,\n          validation_data)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}